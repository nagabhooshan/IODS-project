---
title: "chapter4"
author: "Nagabhooshan Hegde"
date: "19/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#load packages
library(dplyr)
library(ggplot2)
library(corrplot)
library(MASS)
library(GGally)
library(tidyverse)
```


Now load Boston data which in available in MASS library.This data contains Housing Values in Suburbs of Boston. It has 506 observation of 14 variables
More information about this data can be found [here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html) 
```{r}
# Load Boston data
data("Boston")

# Explore the data
str(Boston)
dim(Boston)
```

```{r}
pairs(Boston)
```

Here it is difficult to interpret something as it has many columns.

Now let us see plot matrix of variables
```{r}
ggpairs(Boston[1:14])
```

Now this shows distributions and relationship between variables

Variable rm is normally distributed

biggest correlation between tax and rad variable is (0.910)

Let us use correlation plot to see relationship between variables
```{r}
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits=2) 

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper",cl.pos = "b", tl.pos = "d",tl.cex = 0.6)

```

This shows positive correlation between variables
ex: tax and rad
negative correlation between variables 
ex: dis and age
but some variables doesn't have significant correlation with any other variables
ex: chas


Scaling the data
```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled<- as.data.frame(boston_scaled)

```

Let us create test and train sets
```{r}
# quantile vector crim
bins <- quantile(boston_scaled$crim)
bins
```


```{r}
# Create a categorical variable 'crime':
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low","med_low","med_high","high"))

# Remove original 'crim' from the dataset:
boston_scaled <- dplyr::select(boston_scaled, -crim)

# Add the new categorical value 'crime' to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# choose randomly 80% of the rows:
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)

# create train and test sets:
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
```

Linear Discriminant Analysis(LDA)

Here let's consider Crime as target variable and other variables as predictor variables
```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~., data = train) 
lda.fit 
```

Lets plot LDA
```{r}
# The function for LDA biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime) 
# plotting the lda results and adding arrows
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
```

The above plot explains that LD1 classifies well 
the group having high crime rate.There is a cluster formed in the right side. 

Rad seems significant variable. 

Eventhough LD2 clearly differs between low, med-low, med-high crime rate it cannot clearly form clustors. 
```{r}
# Save the correct classes from test data:
correct_classes <- test$crime

# Remove the crime variable from test data:
test <- dplyr::select(test, -crime)

# Predict classes with test data:
lda.pred <- predict(lda.fit, newdata = test)

# Cross-tabulate the results:
table(correct = correct_classes, predicted = lda.pred$class)
```

Above result shows the model can predict the high crime rate suburbs with high accuracy
It seems difficult to find the difference between med_low and med_high


K means clustering

Reload and standardize Boston data
```{r}
data("Boston")
boston_scaled <- scale(Boston)
boston_scaled=as.data.frame(boston_scaled)
class(boston_scaled)
#Calculate distance matrix between observations
dist_eu <- dist(boston_scaled) 

summary(dist_eu) 
```

Now lets build K means clustoring with this scaled Boston data
```{r}
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

Let us consider 2 clusters 
```{r}
# k-means clustering
km <-kmeans(boston_scaled, centers = 2)

# plot the Boston dataset with clusters
pairs(boston_scaled[1:7], col = km$cluster)
pairs(boston_scaled[7:14], col = km$cluster)
```

From above plots it is clear that some clusters clearly seperated , some are not.
It clearly shows rad variable is most significant
